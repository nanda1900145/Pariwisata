# -*- coding: utf-8 -*-
"""Pariwisata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aFn1zSYWEDlAaXF2HnJuQHQykcEA0KvY
"""

import pandas as pd
import glob
import streamlit as st
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
from zipfile import ZipFile
from pathlib import Path
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import os

# Mendapatkan daftar file CSV di direktori tertentu
file_list = glob.glob('*.csv')
# Membuat dictionary kosong untuk menyimpan data dari setiap file
data_dict = {}
# Melakukan iterasi melalui setiap file dan mengimpor data menggunakan Pandas
for file in file_list:
  # Mendapatkan nama file tanpa ekstensi
  file_name = file.split('/')[-1].split('.')[0]
    
# Mengimpor data dari file CSV
data = pd.read_csv(file)
    
# Menyimpan data dalam dictionary
data_dict[file_name] = data

# Commented out IPython magic to ensure Python compatibility.
# Untuk pengolahan data

# Untuk visualisasi data


# %matplotlib inline
sns.set_palette('Set1')
sns.set()

# Untuk pemodelan

# Untuk menghilangkan warnings saat plotting seaborn


# Untuk mengupload file


rating = pd.read_csv('tourism_rating.csv')
place = pd.read_csv('tourism_with_id.csv')
user = pd.read_csv('user.csv')

rating.head(2)

place.head(2)

user.head(2)

rating.isna().sum()

place.isna().sum()

user.isna().sum()

place = place[place['City']=='Surabaya']
place.head(2)

place.info()

place = place.drop('Time_Minutes', axis=1)

rating = pd.merge(rating, place[['Place_Id']], how='right', on='Place_Id')
rating.head()

#rating.shape

user = pd.merge(user, rating[['User_Id']], how='right', on='User_Id').drop_duplicates().sort_values('User_Id')
user.head()

#user.shape

# Membuat dataframe berisi lokasi dengan jumlah rating terbanyak
top_10 = rating['Place_Id'].value_counts().reset_index()[0:10]
top_10 = pd.merge(top_10, place[['Place_Id','Place_Name']], how='left', left_on='index', right_on='Place_Id')

# Membuat visualisasi wisata dengan jumlah rating terbanyak
plt.figure(figsize=(8,5))
sns.barplot(x='Place_Id_x', y='Place_Name', data=top_10)
plt.title('Jumlah Tempat Wisata dengan Rating Terbanyak', pad=20)
plt.ylabel('Jumlah Rating')
plt.xlabel('Nama Lokasi')
st.pyplot(plt)

# Membuat visualisasi jumlah kategori wisata di Kota Surabaya

sns.countplot(y='Category', data=place)
plt.title('Perbandingan Jumlah Kategori Wisata di Kota Surabaya', pad=20)
st.pyplot(plt)

# Membuat visualisasi distribusi usia user

plt.figure(figsize=(5,3))
sns.boxplot(user['Age']);
plt.title('Distribusi Usia User', pad=20)
st.pyplot(plt)

# Membuat visualisasi distribusi harga masuk tempat wisata

plt.figure(figsize=(7,3))
sns.boxplot(data=place, x='Price')
plt.title('Distribusi Harga Masuk Wisata di Kota Surabaya', pad=20)
st.pyplot(plt)


# Memfilter asal kota dari user
askot = user['Location'].apply(lambda x : x.split(',')[0])

# Visualisasi asal kota dari user
plt.figure(figsize=(8,6))
sns.countplot(y=askot)
plt.title('Jumlah Asal Kota dari User')
st.pyplot(plt)

# Membaca dataset untuk dilakukan encoding
 
df = rating.copy()
df.head()

def dict_encoder(col, data=df):

  # Mengubah kolom suatu dataframe menjadi list tanpa nilai yang sama
  unique_val = data[col].unique().tolist()

  # Melakukan encoding value kolom suatu dataframe ke angka
  val_to_val_encoded = {x: i for i, x in enumerate(unique_val)}

  # Melakukan proses encoding angka ke value dari kolom suatu dataframe
  val_encoded_to_val = {i: x for i, x in enumerate(unique_val)}
  return val_to_val_encoded, val_encoded_to_val

# Encoding User_Id
user_to_user_encoded, user_encoded_to_user = dict_encoder('User_Id')

# Mapping User_Id ke dataframe
df['user'] = df['User_Id'].map(user_to_user_encoded)

# Encoding Place_Id
place_to_place_encoded, place_encoded_to_place = dict_encoder('Place_Id')

# Mapping Place_Id ke dataframe place
df['place'] = df['Place_Id'].map(place_to_place_encoded)

# Mendapatkan jumlah user dan place
num_users, num_place = len(user_to_user_encoded), len(place_to_place_encoded)
 
# Mengubah rating menjadi nilai float
df['Place_Ratings'] = df['Place_Ratings'].values.astype(np.float32)
 
# Mendapatkan nilai minimum dan maksimum rating
min_rating, max_rating = min(df['Place_Ratings']), max(df['Place_Ratings'])
 
st.write(f'Number of User: {num_users}, Number of Place: {num_place}, Min Rating: {min_rating}, Max Rating: {max_rating}')

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df.head(2)

# Membuat variabel x untuk mencocokkan data user dan place menjadi satu value
x = df[['user', 'place']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_places, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_places = num_places
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.places_embedding = layers.Embedding( # layer embeddings places
        num_places,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.places_bias = layers.Embedding(num_places, 1) # layer embedding places bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    places_vector = self.places_embedding(inputs[:, 1]) # memanggil layer embedding 3
    places_bias = self.places_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_places = tf.tensordot(user_vector, places_vector, 2) 
 
    x = dot_user_places + user_bias + places_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_place, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0004),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_root_mean_squared_error')<0.25):
      st.write('Lapor! Metriks validasi sudah sesuai harapan')
      self.model.stop_training = True

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks = [myCallback()]
)

# Menampilkan plot loss dan validation

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.ylim(ymin=0, ymax=0.4)
plt.legend(['train', 'test'], loc='center left')
plt.show()

# Menyiapkan dataframe
place_df = place[['Place_Id','Place_Name','Category','Rating','Price']]
place_df.columns = ['id','place_name','category','rating','price']
df = rating.copy()

# Mengambil sample user
#user_id = df.User_Id.sample(1).iloc[0]
#place_visited_by_user = df[df.User_Id == user_id]

# Membuat data lokasi yang belum dikunjungi user
#place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user.Place_Id.values)]['id'] 
#place_not_visited = list(
#    set(place_not_visited)
#    .intersection(set(place_to_place_encoded.keys()))
#)

model.save('trained_model')



def get_result(user_id, place_not_visited):
  # Mengambil top 10 recommendation
  ratings = model.predict(user_place_array).flatten()
  top_ratings_indices = ratings.argsort()[-10:][::-1]
  recommended_place_ids = [place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices]
     
  st.write('Daftar rekomendasi untuk: {}'.format('User ' + str(user_id)))
  st.write('===' * 15,'\n')
  st.write('----' * 15)
  st.write('Tempat dengan rating wisata paling tinggi dari user')
  st.write('----' * 15)
     
  top_place_user = (
      place_visited_by_user.sort_values(
            by = 'Place_Ratings',
            ascending=False
        )
        .head(5)
        .Place_Id.values
    )
  place_df_rows = place_df[place_df['id'].isin(top_place_user)]
  for row in place_df_rows.itertuples():
    st.write(row.place_name, ':', row.category)

  st.write('')
  st.write('----' * 15)
  st.write('Top 10 place recommendation')
  st.write('----' * 15)
   
  recommended_place = place_df[place_df['id'].isin(recommended_place_ids)]
  for row, i in zip(recommended_place.itertuples(), range(1,11)):
      st.write(i,'.', row.place_name, '\n    ', row.category, ',', 'Harga Tiket Masuk ', row.price, ',', 'Rating Wisata ', row.rating,'\n')

  st.write('==='*15)
  
# Mengambil input user_id
try:
  user_id = int(st.text_input("Masukkan User ID"))
  place_visited_by_user = df[df.User_Id == user_id]
  place_not_visited = place_df[~place_df['id'].isin(place_visited_by_user.Place_Id.values)]['id'] 
  place_not_visited = list(
      set(place_not_visited)
      .intersection(set(place_to_place_encoded.keys()))
  )

  place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
  user_encoder = user_to_user_encoded.get(user_id)
  user_place_array = np.hstack(
      ([[user_encoder]] * len(place_not_visited), place_not_visited)
  )
  get_result(user_id, place_not_visited)
except:
    st.write("Diisi dulu ya")

